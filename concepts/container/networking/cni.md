# Overview

[CNI](https://github.com/containernetworking/cni) is a plug-in based networking interface that defines how container networking should be created and deleted. It is based on the assumption that:

1. The container runtime provides a new network namespace for a new container.
2. The container runtime provides the network configuration for the new container.
3. The container runtime determines which plug-in should be used to configure the new container.

More details can be found at the [CNI spec page](https://github.com/containernetworking/cni/blob/master/SPEC.md)

# Contract

The CNI defines the contract between the runtime and the plug-in. The CNI plug-in is implemented as an executable that is invoked by the container orchestrator (runtime like rkt, or orchestration system like kubernetes).

## Input

The input to the CNI plug-in consists of two parts: runtime parameters and network configurations.

The runtime parameters are passed to the CNI plug-in binary by using environment variables. The following environment variables are used:

    - CNI_COMMAND: indicates the desired operation; ADD, DEL or VERSION.
    - CNI_CONTAINERID: Container ID
    - CNI_NETNS: Path to network namespace file
    - CNI_IFNAME: Interface name to set up; if the plugin is unable to use this interface name it must return an error
    - CNI_ARGS: Extra arguments passed in by the user at invocation time. Alphanumeric key-value pairs separated by semicolons; for example, "FOO=BAR;ABC=123"
    - CNI_PATH: List of paths to search for CNI plugin executables. Paths are separated by an OS-specific list separator; for example ':' on Linux and ';' on Windows
    
The network configurations are passed to the CNI plug-in binary via stdin. It is written in JSON form, and may be stored on disk or generated by runtime.

An example of a network configuration is shown below:

```json
{
  "cniVersion": "0.3.1",
  "name": "my-container-network",
  "type": "bridge",
  "bridge": "docker0",
  "ipam": {
    "type": "host-local",
    "subnet": "10.1.0.0/16",
    "gateway": "10.1.0.1"
  },
  "dns": {
    "nameservers": [ "10.1.0.1" ]
  }
}
```

Network configuration list is a chain of network configurations that the plug-in executes in order. For example:

```json
{
  "cniVersion": "0.3.1",
  "name": "my-container-network",
  "plugins": [
    {
      "type": "bridge",
      "bridge": "docker0",
      "args": {
        "labels" : {
            "appVersion" : "1.0"
        }
      },
      "ipam": {
        "type": "host-local",
        "subnet": "10.1.0.0/16",
        "gateway": "10.1.0.1"
      },
      "dns": {
        "nameservers": [ "10.1.0.1" ]
      }
    },
    {
      "type": "tuning",
      "sysctl": {
        "net.core.somaxconn": "500"
      }
    }
  ]
}
```

An example of executing a CNI plug-in binary is shown below:

```bash
sudo CNI_COMMAND=ADD CNI_CONTAINERID=c60a3769ec74 CNI_NETNS=/var/run/docker/netns/bb266b85d325 CNI_IFNAME=eth0 CNI_PATH=`pwd` ./bridge < mybridge.conf
```

## Output

The output of the plug-in is formed in JSON structure being printed to stdout, as shown below:

```json
{
  "cniVersion": "0.3.1",
  "interfaces": [                                            (this key omitted by IPAM plugins)
      {
          "name": "<name>",
          "mac": "<MAC address>",                            (required if L2 addresses are meaningful)
          "sandbox": "<netns path or hypervisor identifier>" (required for container/hypervisor interfaces, empty/omitted for host interfaces)
      },
      ...
  ],
  "ips": [
      {
          "version": "<4-or-6>",
          "address": "<ip-and-prefix-in-CIDR>",
          "gateway": "<ip-address-of-the-gateway>",          (optional)
          "interface": "<numeric index into 'interfaces' list>"
      },
      ...
  ],
  "routes": [                                                (optional)
      {
          "dst": "<ip-and-prefix-in-cidr>",
          "gw": "<ip-of-next-hop>"                           (optional)
      },
      ...
  ]
  "dns": {
    "nameservers": "<list-of-nameservers>,"                  (optional)
    "domain": "<name-of-local-domain>",                      (optional)
    "search": "<list-of-additional-search-domains>",         (optional)
    "options": "<list-of-options>"                           (optional)
  }
}
```

## Error

Errors must be indicated by a non-zero return code and the following JSON being printed to stdout:

```json
{
  "cniVersion": "0.3.1",
  "code": "<numeric-error-code>",
  "msg": "<short-error-message>",
  "details": "<long-error-message>" (optional)
}
```

# CNI library - libcni

CNI provides a library called [libcni](https://github.com/containernetworking/cni/tree/master/libcni) that is implemented in golang. It wraps the execution of the plug-in binary such that the runtime vendor can directly call the go code.

The library declares an interface that implements five functions:

    - AddNewtwork()
    - DelNetwork()
    - AddNetworkList()
    - DelNetworkList()
    - GetVersionInfo()
    
The five functions map to the CNI commands as discussed above. The runtime needs to pass in the network configuration as well as the runtime parameters. The network configuration is serialized and passed to the binary as stdin. The runtime parameters are converted to environment variables.

For example, let's look at AddNetwork() function.

cni/libcni/api.go
---

```go
func (c *CNIConfig) AddNetwork(net *NetworkConfig, rt *RuntimeConf) (types.Result, error) {
	pluginPath, err := invoke.FindInPath(net.Network.Type, c.Path)
	if err != nil {
		return nil, err
	}

	net, err = injectRuntimeConfig(net, rt)
	if err != nil {
		return nil, err
	}

	return invoke.ExecPluginWithResult(pluginPath, net.Bytes, c.args("ADD", rt))
}
```

Notice how the last line converts the runtime arguments to invoke library arguments:

```go
func (c *CNIConfig) args(action string, rt *RuntimeConf) *invoke.Args {
	return &invoke.Args{
		Command:     action,
		ContainerID: rt.ContainerID,
		NetNS:       rt.NetNS,
		PluginArgs:  rt.Args,
		IfName:      rt.IfName,
		Path:        strings.Join(c.Path, string(os.PathListSeparator)),
	}
}
```

The last line of invoke.ExecPluginWithResult(pluginPath, net.Bytes, c.args("ADD", rt)) calls the invoke library. Internally, it does the parameter conversion:

cni/pkg/invoke/exec.go
---

```go
func (e *PluginExec) WithResult(pluginPath string, netconf []byte, args CNIArgs) (types.Result, error) {
	stdoutBytes, err := e.RawExec.ExecPlugin(pluginPath, netconf, args.AsEnv())
	if err != nil {
		return nil, err
	}

	// Plugin must return result in same version as specified in netconf
	versionDecoder := &version.ConfigDecoder{}
	confVersion, err := versionDecoder.Decode(netconf)
	if err != nil {
		return nil, err
	}

	return version.NewResult(confVersion, stdoutBytes)
}
```

The first line converts runtime arguments to environment variables: 

cni/pkg/invoke/args.go
---

```go
func (args *Args) AsEnv() []string {
	env := os.Environ()
	pluginArgsStr := args.PluginArgsStr
	if pluginArgsStr == "" {
		pluginArgsStr = stringify(args.PluginArgs)
	}

	// Ensure that the custom values are first, so any value present in
	// the process environment won't override them.
	env = append([]string{
		"CNI_COMMAND=" + args.Command,
		"CNI_CONTAINERID=" + args.ContainerID,
		"CNI_NETNS=" + args.NetNS,
		"CNI_ARGS=" + pluginArgsStr,
		"CNI_IFNAME=" + args.IfName,
		"CNI_PATH=" + args.Path,
	}, env...)
	return env
}
```

Then the execution pass in the network configuration through stdin:

cni/pkg/invoke/raw_exec.go
---

```go
func (e *RawExec) ExecPlugin(pluginPath string, stdinData []byte, environ []string) ([]byte, error) {
	stdout := &bytes.Buffer{}

	c := exec.Cmd{
		Env:    environ,
		Path:   pluginPath,
		Args:   []string{pluginPath},
		Stdin:  bytes.NewBuffer(stdinData),
		Stdout: stdout,
		Stderr: e.Stderr,
	}
	if err := c.Run(); err != nil {
		return nil, pluginErr(err, stdout.Bytes())
	}

	return stdout.Bytes(), nil
}
```

# CNI Maintained Plug-In

CNI maintains a list of [default plug-ins](https://github.com/containernetworking/plugins). The following implementations are provided:

Main: interface-creating
---

    - bridge: Creates a bridge, adds the host and the container to it.
    - ipvlan: Adds an ipvlan interface in the container
    - loopback: Creates a loopback interface
    - macvlan: Creates a new MAC address, forwards all traffic to that to the container
    - ptp: Creates a veth pair.
    - vlan: Allocates a vlan device.
    
IPAM: IP address allocation
---

    - dhcp: Runs a daemon on the host to make DHCP requests on behalf of the container
    - host-local: maintains a local database of allocated IPs
    
Meta: other plugins
---

    - flannel: generates an interface corresponding to a flannel config file
    - tuning: Tweaks sysctl parameters of an existing interface
    - portmap: An iptables-based portmapping plugin. Maps ports from the host's address space to the container.

## Bridge Plug-in

Let's take a look at the bridge plug-in.

The cmdAdd() function is responsible to creating a new veth pair to connect the container's network namespace to the host's network namespace. It also creates a new bridge device in the host's namespace, if the given bridge is not present.

First it loads the configuration from stdin.

plugins/plugins/main/bridge/bridge.go
---

```go
func cmdAdd(args *skel.CmdArgs) error {
	n, cniVersion, err := loadNetConf(args.StdinData)
	...
}
```

Next it sets up the bridge. If the bridge is not present, it creates a bridge in the host's namespace.

```go
func cmdAdd(args *skel.CmdArgs) error {
    ...
    
    br, brInterface, err := setupBridge(n)    
    ...    
}
```

```go
func setupBridge(n *NetConf) (*netlink.Bridge, *current.Interface, error) {
	// create bridge if necessary
	br, err := ensureBridge(n.BrName, n.MTU, n.PromiscMode)
	...
}
```

Notice how the following function ignores the error if the bridge is already present.

```go
func ensureBridge(brName string, mtu int, promiscMode bool) (*netlink.Bridge, error) {
	br := &netlink.Bridge{
		LinkAttrs: netlink.LinkAttrs{
			Name: brName,
			MTU:  mtu,
			// Let kernel use default txqueuelen; leaving it unset
			// means 0, and a zero-length TX queue messes up FIFO
			// traffic shapers which use TX queue length as the
			// default packet limit
			TxQLen: -1,
		},
	}

	err := netlink.LinkAdd(br)
	if err != nil && err != syscall.EEXIST {
		return nil, fmt.Errorf("could not add %q: %v", brName, err)
	}

	...
}
```

Next it creates a veth device to connect the container's network namespace and the host's network namespace.

```go
func cmdAdd(args *skel.CmdArgs) error {
	...
	
	netns, err := ns.GetNS(args.Netns)
	...

	hostInterface, containerInterface, err := setupVeth(netns, br, args.IfName, n.MTU, n.HairpinMode)
	...
}
```

It uses the interface name passed by the runtime to configure the container's ethernet interface.

```go
func setupVeth(netns ns.NetNS, br *netlink.Bridge, ifName string, mtu int, hairpinMode bool) (*current.Interface, *current.Interface, error) {
	contIface := &current.Interface{}
	hostIface := &current.Interface{}

	err := netns.Do(func(hostNS ns.NetNS) error {
		// create the veth pair in the container and move host end into host netns
		hostVeth, containerVeth, err := ip.SetupVeth(ifName, mtu, hostNS)
		if err != nil {
			return err
		}
		contIface.Name = containerVeth.Name
		contIface.Mac = containerVeth.HardwareAddr.String()
		contIface.Sandbox = netns.Path()
		hostIface.Name = hostVeth.Name
		return nil
	})
	if err != nil {
		return nil, nil, err
	}

	...

	// connect host veth end to the bridge
	if err := netlink.LinkSetMaster(hostVeth, br); err != nil {
		return nil, nil, fmt.Errorf("failed to connect %q to bridge %v: %v", hostVeth.Attrs().Name, br.Attrs().Name, err)
	}

	...

	return hostIface, contIface, nil
}
```

It delegates the IP allocation to the IPAM plug-in.

```go
func cmdAdd(args *skel.CmdArgs) error {
	...
	
    // run the IPAM plugin and get back the config to apply
    r, err := ipam.ExecAdd(n.IPAM.Type, args.StdinData)
    ...
}
```

The IP and MAC address are configured on the new container device.

```go
func cmdAdd(args *skel.CmdArgs) error {
	...
	
    // Gather gateway information for each IP family
    gwsV4, gwsV6, err := calcGateways(result, n)
    ...
    
    // Configure the container hardware address and IP address(es)
    if err := netns.Do(func(_ ns.NetNS) error {
        contVeth, err := net.InterfaceByName(args.IfName)
        ...

        // Disable IPv6 DAD just in case hairpin mode is enabled on the
        // bridge. Hairpin mode causes echos of neighbor solicitation
        // packets, which causes DAD failures.
        for _, ipc := range result.IPs {
            if ipc.Version == "6" && (n.HairpinMode || n.PromiscMode) {
                if err := disableIPV6DAD(args.IfName); err != nil {
                    return err
                }
                break
            }
        }

        // Add the IP to the interface
        if err := ipam.ConfigureIface(args.IfName, result); err != nil {
            return err
        }

        // Send a gratuitous arp
        for _, ipc := range result.IPs {
            if ipc.Version == "4" {
                _ = arping.GratuitousArpOverIface(ipc.Address.IP, *contVeth)
            }
        }
        return nil
    }); err != nil {
        return err
    }
}
```

## Flannel plug-in

CNI provides a basic flannel plug-in. The plug-in mostly delegates the tasks to the other plug-ins. The flannel plug-in mainly reads the subnet IP information from the local conf, and use that information for IPAM. Details of how flannel manages subnet IPs and how it provides overlay network is discussed in this [flannel document](https://github.com/zeelichsheng/systemdesign/blob/master/concepts/container/networking/flannel.md)

Let's take a look at how the flannel plug-in is implemented.

plugins/plugins/meta/flannel/flannel.go
---

Flannel specific environment variables are stored in the flannel conf file (default to /run/flannel/subnet.env)

    - FLANNEL_NETWORK
    - FLANNEL_SUBNET
    - FLANNEL_MTU
    - FLANNEL_IPMASQ

```go
func cmdAdd(args *skel.CmdArgs) error {
	n, err := loadFlannelNetConf(args.StdinData)
	...

	fenv, err := loadFlannelSubnetEnv(n.SubnetFile)
	...
}
```

Next the plug-in sets up delegates. If not specified in the input, then by default the device management is delegated to "bridge", and IPAM is delegated to "host-local".

```go
func cmdAdd(args *skel.CmdArgs) error {
    ...

	n.Delegate["name"] = n.Name

	if !hasKey(n.Delegate, "type") {
		n.Delegate["type"] = "bridge"
	}
	...

	n.Delegate["ipam"] = map[string]interface{}{
		"type":   "host-local",
		"subnet": fenv.sn.String(),
		"routes": []types.Route{
			types.Route{
				Dst: *fenv.nw,
			},
		},
	}

	return delegateAdd(args.ContainerID, n.DataDir, n.Delegate)
}
```

The delegate's input should look like this after JSON serialization, assuming default delegation:

```json
{
  "cniVersion": "<flannel_cni_version>",
  "name": "<flannel_network_name>",
  "type": "bridge",
  "isBridge": "true",
  "ipMasq": "<inverse of FLANNEL_IPMASQ>",
  "mtu": "<FLANNEL_MTU>",
  "ipam": {
    "type": "host-local",
    "subnet": "<FLANNEL_SUBNET>",
    "routes": {
      "dst": "<FLANNEL_SUBNET>"
    }
  }
}
```